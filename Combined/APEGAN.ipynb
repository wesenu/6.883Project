{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MNIST Data Pre-processing\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils  # utilities for one-hot encoding of ground truth values\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "target = 1\n",
    "\n",
    "x_clean = np.load('../AdvGAN/samples/WB-B-t%d-clean.npy' % target)\n",
    "x_adv = np.load('../AdvGAN/samples/WB-B-t%d-adv.npy' % target)\n",
    "x_label = np.load('../AdvGAN/samples/WB-B-t%d-label.npy' % target)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0 ; Loss [0.27584147, 0.0021352442, 0.31741077]\n",
      "Epoch number: 1 ; Loss [0.15947926, 0.041754942, 0.2272819]\n",
      "Epoch number: 2 ; Loss [0.017294228, 0.004940307, 0.2199826]\n",
      "Epoch number: 3 ; Loss [0.023051597, 0.00039887754, 0.19143327]\n",
      "Epoch number: 4 ; Loss [0.030081943, 0.0016516674, 0.14611138]\n",
      "Epoch number: 5 ; Loss [0.015906662, 0.012311277, 0.11609858]\n",
      "Epoch number: 6 ; Loss [0.007961146, 0.011004613, 0.09629935]\n",
      "Epoch number: 7 ; Loss [0.0073561096, 0.0024783737, 0.07936717]\n",
      "Epoch number: 8 ; Loss [0.006720144, 0.0006554317, 0.06779097]\n",
      "Epoch number: 9 ; Loss [0.007039456, 0.00033149327, 0.062157862]\n",
      "Epoch number: 10 ; Loss [0.009299325, 0.0016464079, 0.056005817]\n",
      "Epoch number: 11 ; Loss [0.005610702, 0.0050847, 0.052139126]\n",
      "Epoch number: 12 ; Loss [0.0050982637, 0.005436663, 0.048606712]\n",
      "Epoch number: 13 ; Loss [0.005209265, 0.0026805722, 0.04906984]\n",
      "Epoch number: 14 ; Loss [0.004750479, 0.0023842698, 0.047452066]\n",
      "Epoch number: 15 ; Loss [0.0038019982, 0.0024695643, 0.047627848]\n",
      "Epoch number: 16 ; Loss [0.0044115167, 0.0021518904, 0.04517719]\n",
      "Epoch number: 17 ; Loss [0.004518937, 0.0021907054, 0.046101954]\n",
      "Epoch number: 18 ; Loss [0.0021479826, 0.002370213, 0.04455405]\n",
      "Epoch number: 19 ; Loss [0.0035108738, 0.003447398, 0.044144038]\n",
      "Epoch number: 20 ; Loss [0.005973169, 0.0016362858, 0.044043478]\n",
      "Epoch number: 21 ; Loss [0.0036827172, 0.0013145956, 0.041528437]\n",
      "Epoch number: 22 ; Loss [0.0036122268, 0.0008472374, 0.041546665]\n",
      "Epoch number: 23 ; Loss [0.0037089256, 0.0011344582, 0.039972037]\n",
      "Epoch number: 24 ; Loss [0.0027863772, 0.0016490275, 0.038352374]\n",
      "Epoch number: 25 ; Loss [0.00043951673, 0.0021999078, 0.036937535]\n",
      "Epoch number: 26 ; Loss [0.0020334045, 0.0012827438, 0.037151553]\n",
      "Epoch number: 27 ; Loss [0.0018930723, 0.001580957, 0.036135457]\n",
      "Epoch number: 28 ; Loss [0.0020905174, 0.000888931, 0.035585597]\n",
      "Epoch number: 29 ; Loss [0.0015240493, 0.0013304343, 0.034108836]\n",
      "Epoch number: 30 ; Loss [0.004938786, 0.0022530016, 0.033921793]\n",
      "Epoch number: 31 ; Loss [0.0013053215, 0.0020123534, 0.034238]\n",
      "Epoch number: 32 ; Loss [0.0012517816, 0.0012613158, 0.034656927]\n",
      "Epoch number: 33 ; Loss [0.0011416583, 0.0013706612, 0.033859063]\n",
      "Epoch number: 34 ; Loss [0.0018599594, 0.0022748937, 0.033258952]\n",
      "Epoch number: 35 ; Loss [0.0015218755, 0.0023015942, 0.03345816]\n",
      "Epoch number: 36 ; Loss [0.002176293, 0.0019651428, 0.03311174]\n",
      "Epoch number: 37 ; Loss [0.0016876375, 0.0009539397, 0.034285497]\n",
      "Epoch number: 38 ; Loss [0.0022490118, 0.0014323955, 0.032422386]\n",
      "Epoch number: 39 ; Loss [0.0022890365, 0.0024399117, 0.032400392]\n",
      "Epoch number: 40 ; Loss [0.0016786279, 0.0013748355, 0.03364986]\n",
      "Epoch number: 41 ; Loss [0.0016742336, 0.0010353187, 0.032734986]\n",
      "Epoch number: 42 ; Loss [0.0014692256, 0.0025270777, 0.032102447]\n",
      "Epoch number: 43 ; Loss [0.0018465326, 0.0011875518, 0.0325459]\n",
      "Epoch number: 44 ; Loss [0.00072521716, 0.0012087486, 0.031993147]\n",
      "Epoch number: 45 ; Loss [0.0025601457, 0.0018480883, 0.03162397]\n",
      "Epoch number: 46 ; Loss [0.0008110332, 0.0012710106, 0.03134586]\n",
      "Epoch number: 47 ; Loss [0.0014168932, 0.0017684632, 0.03085697]\n",
      "Epoch number: 48 ; Loss [0.0014574114, 0.0015307822, 0.030506238]\n",
      "Epoch number: 49 ; Loss [0.0006488155, 0.00068674714, 0.030665942]\n",
      "Epoch number: 50 ; Loss [0.001021363, 0.0007569533, 0.030160666]\n",
      "Epoch number: 51 ; Loss [0.0012808546, 0.0016807852, 0.028528215]\n",
      "Epoch number: 52 ; Loss [0.0011576506, 0.0017907564, 0.028548757]\n",
      "Epoch number: 53 ; Loss [0.0021766461, 0.0008721369, 0.028666133]\n",
      "Epoch number: 54 ; Loss [0.0022509005, 0.0005364257, 0.028088436]\n",
      "Epoch number: 55 ; Loss [0.0010046251, 0.0016139605, 0.02851753]\n",
      "Epoch number: 56 ; Loss [0.00095004, 0.0014745627, 0.02730004]\n",
      "Epoch number: 57 ; Loss [0.00097527105, 0.0011435974, 0.028419832]\n",
      "Epoch number: 58 ; Loss [0.0006226506, 0.001054179, 0.02825316]\n",
      "Epoch number: 59 ; Loss [0.00020243265, 0.0012427771, 0.02823969]\n",
      "Epoch number: 60 ; Loss [0.0015782509, 0.0016690681, 0.028174076]\n",
      "Epoch number: 61 ; Loss [0.0007651726, 0.00057154085, 0.029081091]\n",
      "Epoch number: 62 ; Loss [0.00028425932, 0.0010721952, 0.027767112]\n",
      "Epoch number: 63 ; Loss [0.0008288912, 0.002005059, 0.027076112]\n",
      "Epoch number: 64 ; Loss [0.0012098765, 0.0012709849, 0.02773405]\n",
      "Epoch number: 65 ; Loss [0.0003615978, 0.00054593885, 0.027127834]\n",
      "Epoch number: 66 ; Loss [0.0016265016, 0.001973094, 0.027673738]\n",
      "Epoch number: 67 ; Loss [0.00038123594, 0.00082099333, 0.027885105]\n",
      "Epoch number: 68 ; Loss [0.0006808894, 0.0017374736, 0.027040927]\n",
      "Epoch number: 69 ; Loss [0.00047300992, 0.0010691211, 0.027248865]\n",
      "Epoch number: 70 ; Loss [0.00029919186, 0.0007043232, 0.027770145]\n",
      "Epoch number: 71 ; Loss [0.0009688109, 0.0015130024, 0.026171219]\n",
      "Epoch number: 72 ; Loss [0.0028088004, 0.0010515797, 0.027242161]\n",
      "Epoch number: 73 ; Loss [0.00068998395, 0.0012576398, 0.026663473]\n",
      "Epoch number: 74 ; Loss [0.0004806674, 0.0009875336, 0.026243525]\n",
      "Epoch number: 75 ; Loss [0.0004895711, 0.0011177987, 0.026164562]\n",
      "Epoch number: 76 ; Loss [0.00021512172, 0.0011616687, 0.027028326]\n",
      "Epoch number: 77 ; Loss [0.0018133032, 0.0012700299, 0.026770363]\n",
      "Epoch number: 78 ; Loss [0.0016800568, 0.0008460487, 0.026442405]\n",
      "Epoch number: 79 ; Loss [0.00053997, 0.00089977874, 0.02628584]\n",
      "Epoch number: 80 ; Loss [0.00081871933, 0.0020083473, 0.025567606]\n",
      "Epoch number: 81 ; Loss [0.001280083, 0.00063411595, 0.027403815]\n",
      "Epoch number: 82 ; Loss [0.0011971501, 0.0008557471, 0.026244689]\n",
      "Epoch number: 83 ; Loss [0.0013184852, 0.0018066012, 0.02605486]\n",
      "Epoch number: 84 ; Loss [0.0011895628, 0.0008966836, 0.025746657]\n",
      "Epoch number: 85 ; Loss [0.0005904213, 0.0008126767, 0.02651281]\n",
      "Epoch number: 86 ; Loss [0.0008852936, 0.0017583047, 0.025659004]\n",
      "Epoch number: 87 ; Loss [0.00032635956, 0.0008957153, 0.025797375]\n",
      "Epoch number: 88 ; Loss [0.0005693405, 0.0007470968, 0.026059499]\n",
      "Epoch number: 89 ; Loss [0.00015763749, 0.0010773086, 0.026415806]\n",
      "Epoch number: 90 ; Loss [0.0012088061, 0.0015708169, 0.025396556]\n",
      "Epoch number: 91 ; Loss [0.0011463006, 0.0007019518, 0.025580473]\n",
      "Epoch number: 92 ; Loss [0.0011264586, 0.0007887244, 0.025887232]\n",
      "Epoch number: 93 ; Loss [0.0009155164, 0.0012931095, 0.024784919]\n",
      "Epoch number: 94 ; Loss [0.001457174, 0.0011671642, 0.024770282]\n",
      "Epoch number: 95 ; Loss [0.0026714874, 0.0005601426, 0.026008608]\n",
      "Epoch number: 96 ; Loss [0.00010714546, 0.0011912913, 0.02478181]\n",
      "Epoch number: 97 ; Loss [0.00018077157, 0.0010322074, 0.025307516]\n",
      "Epoch number: 98 ; Loss [0.0009091907, 0.0012653894, 0.024935644]\n",
      "Epoch number: 99 ; Loss [0.00044797146, 0.0007100068, 0.024987495]\n"
     ]
    }
   ],
   "source": [
    "from APEGAN import APEGAN\n",
    "\n",
    "GAN, G, D = APEGAN([28,28,1])\n",
    "\n",
    "epochs=100\n",
    "batch_size=512\n",
    "\n",
    "np.random.shuffle(x_adv)\n",
    "np.random.shuffle(x_clean)\n",
    "\n",
    "N = x_clean.shape[0]\n",
    "scalarloss = [0,0,0]\n",
    "start = 0\n",
    "for cur_epoch in range(epochs):\n",
    "    idx = range(start, start+batch_size)\n",
    "    start += batch_size\n",
    "    if start > N-batch_size:\n",
    "        start = 0\n",
    "    x_clean_batch = x_clean[idx,]\n",
    "    x_adv_batch = x_adv[idx,]\n",
    "    x_batch = np.concatenate((x_clean_batch, x_adv_batch), axis=0)\n",
    "    scalarloss[0] = D.train_on_batch(x_batch, np.concatenate((np.ones([batch_size,1]), np.zeros([batch_size,1])), axis=0))\n",
    "    GAN.train_on_batch(x_adv_batch, [np.ones([batch_size,1]), x_adv_batch])[1:]\n",
    "    scalarloss[1:] = GAN.train_on_batch(x_adv_batch, [np.ones([batch_size,1]), x_adv_batch])[1:]\n",
    "    print(\"Epoch number:\",cur_epoch,\"; Loss\",scalarloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45165807\n",
      "0.36795855\n",
      "0.5094911\n",
      "0.5141288\n"
     ]
    }
   ],
   "source": [
    "test_range = x_adv.shape[0]\n",
    "adv = x_adv[:test_range]\n",
    "clean = x_clean[:test_range]\n",
    "\n",
    "print(np.mean(GAN.predict(adv)[0]))\n",
    "print(np.mean(D.predict(adv)))\n",
    "print(np.mean(GAN.predict(clean)[0]))\n",
    "print(np.mean(D.predict(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv acc:0.0740, rct acc:0.1040\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "F = keras.models.load_model('../AdvGAN/models/Classifier-' + 'A' + '.h5')\n",
    "\n",
    "test_range = x_adv.shape[0]\n",
    "adv = x_adv[:test_range]\n",
    "label = x_label[:test_range]\n",
    "purified = G.predict(adv)\n",
    "adv_pdt = np.argmax(F.predict(adv), axis=1)\n",
    "purified_pdt = np.argmax(F.predict(purified), axis=1)\n",
    "\n",
    "print('adv acc:{:.4f}, rct acc:{:.4f}'.format(np.sum(adv_pdt==label)/test_range, \n",
    "                                              np.sum(purified_pdt==label)/test_range))\n",
    "\n",
    "for k in range(0):\n",
    "    print(adv_pdt[k])\n",
    "    plt.imshow((adv[k,] * 255).astype(np.int).reshape(28,28), cmap='gray')\n",
    "    plt.show()\n",
    "    print(purified_pdt[k])\n",
    "    plt.imshow((purified[k,] * 255).astype(np.int).reshape(28,28), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
