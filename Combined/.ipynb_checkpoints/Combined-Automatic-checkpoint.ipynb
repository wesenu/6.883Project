{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MNIST Data Pre-processing\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils  # utilities for one-hot encoding of ground truth values\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = np_utils.to_categorical(y_train, 10).astype(np.float32)\n",
    "y_test = np_utils.to_categorical(y_test, 10).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:282: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, A : adv acc:0.0907, rct d acc:0.9755, rct adv acc:0.4312\n",
      "2, B : adv acc:0.0411, rct d acc:0.9798, rct adv acc:0.5980\n",
      "2, C : adv acc:0.2630, rct d acc:0.9787, rct adv acc:0.6309\n",
      "3, A : adv acc:0.2388, rct d acc:0.8389, rct adv acc:0.2328\n",
      "3, B : adv acc:0.0197, rct d acc:0.8122, rct adv acc:0.0643\n",
      "3, C : adv acc:0.1605, rct d acc:0.8416, rct adv acc:0.1570\n",
      "4, A : adv acc:0.3907, rct d acc:0.9502, rct adv acc:0.4809\n",
      "4, B : adv acc:0.0122, rct d acc:0.8901, rct adv acc:0.2070\n",
      "4, C : adv acc:0.3933, rct d acc:0.9162, rct adv acc:0.4284\n",
      "5, A : adv acc:0.0414, rct d acc:0.6634, rct adv acc:0.1078\n",
      "5, B : adv acc:0.0054, rct d acc:0.7760, rct adv acc:0.0580\n",
      "5, C : adv acc:0.0546, rct d acc:0.6856, rct adv acc:0.1072\n",
      "6, A : adv acc:0.2637, rct d acc:0.7597, rct adv acc:0.2344\n",
      "6, B : adv acc:0.0216, rct d acc:0.7715, rct adv acc:0.0513\n",
      "6, C : adv acc:0.2214, rct d acc:0.7264, rct adv acc:0.1821\n",
      "7, A : adv acc:0.2536, rct d acc:0.6486, rct adv acc:0.4080\n",
      "7, B : adv acc:0.0138, rct d acc:0.6516, rct adv acc:0.2137\n",
      "7, C : adv acc:0.2331, rct d acc:0.6996, rct adv acc:0.4257\n",
      "8, A : adv acc:0.0102, rct d acc:0.8142, rct adv acc:0.0307\n",
      "8, B : adv acc:0.0027, rct d acc:0.8218, rct adv acc:0.0400\n",
      "8, C : adv acc:0.0369, rct d acc:0.8283, rct adv acc:0.0258\n",
      "9, A : adv acc:0.1959, rct d acc:0.6781, rct adv acc:0.3119\n",
      "9, B : adv acc:0.0057, rct d acc:0.6020, rct adv acc:0.1762\n",
      "9, C : adv acc:0.1369, rct d acc:0.6210, rct adv acc:0.2454\n"
     ]
    }
   ],
   "source": [
    "from Combined import Combined\n",
    "\n",
    "GAN, G, D, F = Combined([28,28,1], 'B')\n",
    "\n",
    "epochs=100\n",
    "batch_size=256\n",
    "\n",
    "for target in range(2, 10):\n",
    "    scalarloss = [0,0,0,0,0,0,0]\n",
    "    for cur_epoch in range(epochs):\n",
    "        for discrm_epoch in range(4):\n",
    "            input_batch = x_train[np.random.randint(0, x_train.shape[0], size=int(batch_size)),]\n",
    "            if discrm_epoch % 2 == 1:\n",
    "                y_discrim = np.ones([batch_size,1])\n",
    "                scalarloss[:2] = D.train_on_batch(input_batch, [y_discrim, input_batch])[1:]\n",
    "            else:\n",
    "                input_batch = np.add(input_batch, G.predict(input_batch))\n",
    "                y_discrim = np.zeros([batch_size,1])\n",
    "                scalarloss[2:4] = D.train_on_batch(input_batch, [y_discrim, input_batch])[1:]\n",
    "\n",
    "        input_idx = np.random.randint(0, x_train.shape[0], size=int(batch_size))\n",
    "        input_batch = x_train[input_idx,]\n",
    "        y_discrim = np.ones([batch_size,1])\n",
    "        y_class=np_utils.to_categorical(np.ones(batch_size)*target, 10).astype(np.float32)\n",
    "        y_hinge=np.zeros([batch_size,28,28,1])\n",
    "        scalarloss[4:] = GAN.train_on_batch(input_batch, [y_discrim, y_class, y_hinge])[1:]\n",
    "        #print(\"Epoch number:\",cur_epoch,\"; Loss\",scalarloss)\n",
    "\n",
    "    clean = x_test\n",
    "    adv = clean + G.predict(clean)\n",
    "    label = np.argmax(y_test, axis=1)\n",
    "    idx = (label != target)\n",
    "    clean = clean[idx,]\n",
    "    adv = adv[idx,]\n",
    "    label = label[idx,]\n",
    "    classifier_name = ['A', 'B', 'C']\n",
    "    APEADVG = keras.models.load_model('./models/APEGAN-AdvGANAllTarget-G.h5')\n",
    "    APECWG = keras.models.load_model('./models/APEGAN-CW-G.h5')\n",
    "    for cn in classifier_name:\n",
    "        BF = keras.models.load_model('../AdvGAN/models/Classifier-' + cn + '.h5')\n",
    "        adv_pdt = np.argmax(BF.predict(adv), axis=1)\n",
    "        purified1_pdt = np.argmax(BF.predict(APEADVG.predict(adv)), axis=1)\n",
    "        purified2_pdt = np.argmax(BF.predict(APECWG.predict(adv)), axis=1)\n",
    "        print('{}, {} : adv acc:{:.4f}, rct d acc:{:.4f}, rct adv acc:{:.4f}'.format(\n",
    "            target, cn, np.mean(adv_pdt==label), np.mean(purified1_pdt==label),\n",
    "            np.mean(purified2_pdt==label)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
