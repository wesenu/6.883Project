{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MNIST Data Pre-processing\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils  # utilities for one-hot encoding of ground truth values\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = np_utils.to_categorical(y_train, 10).astype(np.float32)\n",
    "y_test = np_utils.to_categorical(y_test, 10).astype(np.float32)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0 ; Loss [1.6756883, 0.5927419, 0.7996539, 0.28329253]\n",
      "Epoch number: 1 ; Loss [1.5775757, 0.5030103, 0.83166766, 0.24289778]\n",
      "Epoch number: 2 ; Loss [1.4945205, 0.43376505, 0.8022766, 0.25847888]\n",
      "Epoch number: 3 ; Loss [1.3425117, 0.3429004, 0.7314091, 0.26820216]\n",
      "Epoch number: 4 ; Loss [1.409597, 0.35651648, 0.7720194, 0.28106114]\n",
      "Epoch number: 5 ; Loss [1.5872289, 0.5194827, 0.79171187, 0.27603436]\n",
      "Epoch number: 6 ; Loss [1.5858396, 0.59737635, 0.74072444, 0.24773882]\n",
      "Epoch number: 7 ; Loss [1.5987488, 0.59792095, 0.7651876, 0.23564017]\n",
      "Epoch number: 8 ; Loss [1.5302902, 0.5706174, 0.7304355, 0.2292374]\n",
      "Epoch number: 9 ; Loss [1.6770511, 0.63459903, 0.8114723, 0.23097962]\n",
      "Epoch number: 10 ; Loss [1.7313169, 0.7403955, 0.7528987, 0.23802266]\n",
      "Epoch number: 11 ; Loss [1.8520292, 0.8283625, 0.78083855, 0.24282813]\n",
      "Epoch number: 12 ; Loss [1.9033883, 0.8576993, 0.8069248, 0.23876415]\n",
      "Epoch number: 13 ; Loss [1.8721884, 0.8583147, 0.78137785, 0.23249587]\n",
      "Epoch number: 14 ; Loss [1.8318846, 0.8475549, 0.758656, 0.2256737]\n",
      "Epoch number: 15 ; Loss [1.8781779, 0.84581804, 0.81842583, 0.21393393]\n",
      "Epoch number: 16 ; Loss [1.8464351, 0.8594323, 0.7826901, 0.20431271]\n",
      "Epoch number: 17 ; Loss [1.8611304, 0.8692105, 0.8028923, 0.18902758]\n",
      "Epoch number: 18 ; Loss [1.7808923, 0.8838509, 0.72977614, 0.16726533]\n",
      "Epoch number: 19 ; Loss [1.7917451, 0.906475, 0.74266934, 0.14260069]\n",
      "Epoch number: 20 ; Loss [1.8166145, 0.91559976, 0.76770186, 0.13331279]\n",
      "Epoch number: 21 ; Loss [1.7459917, 0.91537446, 0.6920049, 0.13861221]\n",
      "Epoch number: 22 ; Loss [1.7405993, 0.8963969, 0.6993114, 0.144891]\n",
      "Epoch number: 23 ; Loss [1.68325, 0.8885423, 0.6421454, 0.15256225]\n",
      "Epoch number: 24 ; Loss [1.6643188, 0.90996724, 0.5878409, 0.16651058]\n",
      "Epoch number: 25 ; Loss [1.6904535, 0.92722046, 0.5835943, 0.17963874]\n",
      "Epoch number: 26 ; Loss [1.7205412, 0.9317014, 0.6013238, 0.18751602]\n",
      "Epoch number: 27 ; Loss [1.7106644, 0.93216765, 0.58486754, 0.19362918]\n",
      "Epoch number: 28 ; Loss [1.7202554, 0.94312125, 0.5757652, 0.20136897]\n",
      "Epoch number: 29 ; Loss [1.6345689, 0.9462662, 0.48720795, 0.20109485]\n",
      "Epoch number: 30 ; Loss [1.6548028, 0.95051193, 0.5043183, 0.19997266]\n",
      "Epoch number: 31 ; Loss [1.6037804, 0.95680356, 0.4509626, 0.19601424]\n",
      "Epoch number: 32 ; Loss [1.6081556, 0.9617646, 0.45931816, 0.18707283]\n",
      "Epoch number: 33 ; Loss [1.5373335, 0.9677055, 0.39213765, 0.17749035]\n",
      "Epoch number: 34 ; Loss [1.6172249, 0.97021234, 0.48523435, 0.16177815]\n",
      "Epoch number: 35 ; Loss [1.4876616, 0.97145295, 0.36788237, 0.14832631]\n",
      "Epoch number: 36 ; Loss [1.535131, 0.97195566, 0.423505, 0.13967037]\n",
      "Epoch number: 37 ; Loss [1.5252849, 0.9718004, 0.41996875, 0.13351573]\n",
      "Epoch number: 38 ; Loss [1.4584179, 0.97222906, 0.36150986, 0.12467902]\n",
      "Epoch number: 39 ; Loss [1.4128162, 0.97109544, 0.3233376, 0.118383095]\n",
      "Epoch number: 40 ; Loss [1.4841601, 0.9677379, 0.40110922, 0.115312986]\n",
      "Epoch number: 41 ; Loss [1.4456542, 0.96588874, 0.3645457, 0.11521977]\n",
      "Epoch number: 42 ; Loss [1.4335647, 0.963025, 0.34996748, 0.12057225]\n",
      "Epoch number: 43 ; Loss [1.4265513, 0.9603329, 0.3352493, 0.13096923]\n",
      "Epoch number: 44 ; Loss [1.3892845, 0.9548623, 0.29789937, 0.13652289]\n",
      "Epoch number: 45 ; Loss [1.4377404, 0.94784, 0.3536442, 0.13625638]\n",
      "Epoch number: 46 ; Loss [1.3879206, 0.9363236, 0.31731784, 0.13427927]\n",
      "Epoch number: 47 ; Loss [1.3821161, 0.9289242, 0.3200075, 0.13318439]\n",
      "Epoch number: 48 ; Loss [1.3802953, 0.92274904, 0.3169769, 0.14056936]\n",
      "Epoch number: 49 ; Loss [1.2567054, 0.9196084, 0.17767553, 0.15942147]\n",
      "Epoch number: 50 ; Loss [1.2940633, 0.9192371, 0.19778699, 0.17703927]\n",
      "Epoch number: 51 ; Loss [1.3664321, 0.9193541, 0.25190872, 0.19516934]\n",
      "Epoch number: 52 ; Loss [1.3236413, 0.9208236, 0.19413242, 0.20868531]\n",
      "Epoch number: 53 ; Loss [1.3271847, 0.9190967, 0.18528014, 0.22280791]\n",
      "Epoch number: 54 ; Loss [1.2958502, 0.9108131, 0.15591511, 0.22912194]\n",
      "Epoch number: 55 ; Loss [1.3149772, 0.91586435, 0.16330507, 0.2358078]\n",
      "Epoch number: 56 ; Loss [1.3641922, 0.92862165, 0.18958442, 0.24598628]\n",
      "Epoch number: 57 ; Loss [1.2864398, 0.93482065, 0.10094607, 0.25067303]\n",
      "Epoch number: 58 ; Loss [1.2374952, 0.939155, 0.051435817, 0.24690439]\n",
      "Epoch number: 59 ; Loss [1.260296, 0.9307666, 0.09389087, 0.23563847]\n",
      "Epoch number: 60 ; Loss [1.2273062, 0.9312974, 0.07430894, 0.22169979]\n",
      "Epoch number: 61 ; Loss [1.2092129, 0.9366673, 0.070074014, 0.20247164]\n",
      "Epoch number: 62 ; Loss [1.2708136, 0.9413421, 0.14837897, 0.18109253]\n",
      "Epoch number: 63 ; Loss [1.2598734, 0.9385677, 0.15208155, 0.16922411]\n",
      "Epoch number: 64 ; Loss [1.2204118, 0.95093745, 0.107086, 0.16238832]\n",
      "Epoch number: 65 ; Loss [1.2054548, 0.9586589, 0.08615121, 0.16064481]\n",
      "Epoch number: 66 ; Loss [1.2479308, 0.96342295, 0.13001114, 0.15449673]\n",
      "Epoch number: 67 ; Loss [1.24101, 0.9624837, 0.12046751, 0.15805876]\n",
      "Epoch number: 68 ; Loss [1.1735549, 0.954772, 0.062808156, 0.1559747]\n",
      "Epoch number: 69 ; Loss [1.1622062, 0.95171165, 0.06039813, 0.15009639]\n",
      "Epoch number: 70 ; Loss [1.181969, 0.9311242, 0.1037279, 0.14711685]\n",
      "Epoch number: 71 ; Loss [1.306258, 0.973302, 0.18274054, 0.15021533]\n",
      "Epoch number: 72 ; Loss [1.269688, 0.9886459, 0.12676263, 0.15427947]\n",
      "Epoch number: 73 ; Loss [1.2440679, 0.9921459, 0.10031382, 0.15160817]\n",
      "Epoch number: 74 ; Loss [1.2018409, 0.9914981, 0.054346602, 0.15599617]\n",
      "Epoch number: 75 ; Loss [1.2096007, 0.9904386, 0.06452464, 0.15463749]\n",
      "Epoch number: 76 ; Loss [1.2174841, 0.9913307, 0.07018439, 0.15596902]\n",
      "Epoch number: 77 ; Loss [1.1950271, 0.9910877, 0.05106698, 0.1528725]\n",
      "Epoch number: 78 ; Loss [1.1735947, 0.9915021, 0.038224358, 0.14386827]\n",
      "Epoch number: 79 ; Loss [1.1899035, 0.9912386, 0.06695473, 0.13171013]\n",
      "Epoch number: 80 ; Loss [1.1849906, 0.9902763, 0.07380889, 0.12090551]\n",
      "Epoch number: 81 ; Loss [1.2034261, 0.98994416, 0.10538583, 0.10809609]\n",
      "Epoch number: 82 ; Loss [1.182631, 0.9891449, 0.09169426, 0.10179192]\n",
      "Epoch number: 83 ; Loss [1.1598021, 0.98714614, 0.06942391, 0.10323207]\n",
      "Epoch number: 84 ; Loss [1.1370257, 0.9873396, 0.039632138, 0.110053964]\n",
      "Epoch number: 85 ; Loss [1.1405412, 0.9866216, 0.03620728, 0.117712274]\n",
      "Epoch number: 86 ; Loss [1.1387413, 0.9875329, 0.026861545, 0.124346875]\n",
      "Epoch number: 87 ; Loss [1.1229469, 0.9849452, 0.012549478, 0.12545225]\n",
      "Epoch number: 88 ; Loss [1.1478877, 0.98427016, 0.04021821, 0.12339936]\n",
      "Epoch number: 89 ; Loss [1.1468877, 0.9852467, 0.04486311, 0.11677778]\n",
      "Epoch number: 90 ; Loss [1.1283959, 0.9827683, 0.03446252, 0.11116504]\n",
      "Epoch number: 91 ; Loss [1.1107373, 0.98297995, 0.022932157, 0.10482529]\n",
      "Epoch number: 92 ; Loss [1.1275777, 0.98363864, 0.044393726, 0.099545226]\n",
      "Epoch number: 93 ; Loss [1.1203302, 0.9838012, 0.041942295, 0.09458676]\n",
      "Epoch number: 94 ; Loss [1.1026573, 0.98180985, 0.03216879, 0.08867878]\n",
      "Epoch number: 95 ; Loss [1.1074365, 0.97996104, 0.042088125, 0.08538735]\n",
      "Epoch number: 96 ; Loss [1.1152998, 0.97883344, 0.052426543, 0.08403979]\n",
      "Epoch number: 97 ; Loss [1.1156996, 0.97926617, 0.053485, 0.08294849]\n",
      "Epoch number: 98 ; Loss [1.1231533, 0.97993803, 0.05672198, 0.08649334]\n",
      "Epoch number: 99 ; Loss [1.1059023, 0.9793589, 0.035373542, 0.09116981]\n",
      "Epoch number: 100 ; Loss [1.1109152, 0.9770801, 0.03801677, 0.09581823]\n",
      "Epoch number: 101 ; Loss [1.0778248, 0.97553813, 0.0019112804, 0.10037537]\n",
      "Epoch number: 102 ; Loss [1.1019427, 0.97539455, 0.022613341, 0.1039348]\n",
      "Epoch number: 103 ; Loss [1.0959164, 0.9694646, 0.022459542, 0.103992224]\n",
      "Epoch number: 104 ; Loss [1.0997685, 0.9642194, 0.032957915, 0.10259123]\n",
      "Epoch number: 105 ; Loss [1.1029449, 0.9592271, 0.046042655, 0.09767513]\n",
      "Epoch number: 106 ; Loss [1.0619402, 0.96042466, 0.0052346354, 0.09628092]\n",
      "Epoch number: 107 ; Loss [1.0874504, 0.9559349, 0.039400633, 0.09211482]\n",
      "Epoch number: 108 ; Loss [1.119056, 0.9694132, 0.0578785, 0.091764174]\n",
      "Epoch number: 109 ; Loss [1.1167463, 0.97749466, 0.046490975, 0.09276069]\n",
      "Epoch number: 110 ; Loss [1.1013138, 0.9822725, 0.0235617, 0.095479615]\n",
      "Epoch number: 111 ; Loss [1.0977585, 0.9876969, 0.012585964, 0.09747563]\n",
      "Epoch number: 112 ; Loss [1.1089278, 0.9868231, 0.026550991, 0.09555374]\n",
      "Epoch number: 113 ; Loss [1.0840588, 0.98822546, 0.0006110023, 0.09522225]\n",
      "Epoch number: 114 ; Loss [1.096925, 0.98465836, 0.022188818, 0.09007793]\n",
      "Epoch number: 115 ; Loss [1.0903821, 0.9843029, 0.018907677, 0.087171584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 116 ; Loss [1.0828899, 0.9796621, 0.023051891, 0.08017586]\n",
      "Epoch number: 117 ; Loss [1.0964432, 0.9711236, 0.05204007, 0.073279455]\n",
      "Epoch number: 118 ; Loss [1.0746638, 0.9614287, 0.044274855, 0.06896024]\n",
      "Epoch number: 119 ; Loss [1.0854676, 0.96561944, 0.04779742, 0.07205065]\n",
      "Epoch number: 120 ; Loss [1.0848761, 0.97091, 0.035216372, 0.07874961]\n",
      "Epoch number: 121 ; Loss [1.0834572, 0.97476935, 0.018992774, 0.08969511]\n",
      "Epoch number: 122 ; Loss [1.0999328, 0.98156226, 0.01709494, 0.10127555]\n",
      "Epoch number: 123 ; Loss [1.1079985, 0.97936577, 0.021946287, 0.10668652]\n",
      "Epoch number: 124 ; Loss [1.1078476, 0.9718713, 0.025957685, 0.11001851]\n",
      "Epoch number: 125 ; Loss [1.0979592, 0.97049093, 0.016898649, 0.11056956]\n",
      "Epoch number: 126 ; Loss [1.0900297, 0.9735369, 0.012281504, 0.10421127]\n",
      "Epoch number: 127 ; Loss [1.1111104, 0.9754123, 0.03880682, 0.09689124]\n",
      "Epoch number: 128 ; Loss [1.0946964, 0.98122895, 0.020358525, 0.09310891]\n",
      "Epoch number: 129 ; Loss [1.0890746, 0.98775613, 0.013582209, 0.0877362]\n",
      "Epoch number: 130 ; Loss [1.0799214, 0.9823349, 0.012326507, 0.08526001]\n",
      "Epoch number: 131 ; Loss [1.1071556, 0.9862822, 0.03633116, 0.08454211]\n",
      "Epoch number: 132 ; Loss [1.0716887, 0.98740673, 0.004697591, 0.07958439]\n",
      "Epoch number: 133 ; Loss [1.087133, 0.9867978, 0.0226406, 0.077694654]\n",
      "Epoch number: 134 ; Loss [1.0867406, 0.98927, 0.022069376, 0.075401336]\n",
      "Epoch number: 135 ; Loss [1.0784447, 0.99148935, 0.0068882154, 0.08006711]\n",
      "Epoch number: 136 ; Loss [1.100493, 0.99108714, 0.027118322, 0.08228761]\n",
      "Epoch number: 137 ; Loss [1.0984542, 0.9896598, 0.023008866, 0.085785635]\n",
      "Epoch number: 138 ; Loss [1.1015859, 0.9885209, 0.033170737, 0.079894215]\n",
      "Epoch number: 139 ; Loss [1.0974559, 0.9919679, 0.03012833, 0.075359575]\n",
      "Epoch number: 140 ; Loss [1.0749905, 0.9945206, 0.0036143877, 0.07685558]\n",
      "Epoch number: 141 ; Loss [1.0848222, 0.99556285, 0.009675921, 0.07958343]\n",
      "Epoch number: 142 ; Loss [1.0824916, 0.99719083, 0.006395923, 0.07890488]\n",
      "Epoch number: 143 ; Loss [1.0855752, 0.99517286, 0.0186092, 0.07179321]\n",
      "Epoch number: 144 ; Loss [1.0737073, 0.99522614, 0.011674576, 0.06680672]\n",
      "Epoch number: 145 ; Loss [1.080254, 0.9934914, 0.028230434, 0.05853212]\n",
      "Epoch number: 146 ; Loss [1.0829389, 0.989112, 0.036816064, 0.057010792]\n",
      "Epoch number: 147 ; Loss [1.0649115, 0.9888849, 0.018106118, 0.057920486]\n",
      "Epoch number: 148 ; Loss [1.0732131, 0.9886141, 0.027562179, 0.05703687]\n",
      "Epoch number: 149 ; Loss [1.0611479, 0.9928192, 0.012393955, 0.055934783]\n",
      "Epoch number: 150 ; Loss [1.079628, 0.99353224, 0.032867283, 0.053228483]\n",
      "Epoch number: 151 ; Loss [1.0594629, 0.9931913, 0.0077835573, 0.058488034]\n",
      "Epoch number: 152 ; Loss [1.0596882, 0.99251056, 0.0015969069, 0.06558074]\n",
      "Epoch number: 153 ; Loss [1.0804374, 0.9929193, 0.022262046, 0.065255985]\n",
      "Epoch number: 154 ; Loss [1.0774616, 0.99153167, 0.01967492, 0.06625493]\n",
      "Epoch number: 155 ; Loss [1.0547276, 0.9880488, 0.0032832106, 0.0633956]\n",
      "Epoch number: 156 ; Loss [1.0732747, 0.9852948, 0.030128062, 0.057851933]\n",
      "Epoch number: 157 ; Loss [1.067947, 0.99048233, 0.018445302, 0.05901947]\n",
      "Epoch number: 158 ; Loss [1.0700151, 0.99362063, 0.016121253, 0.06027317]\n",
      "Epoch number: 159 ; Loss [1.0697366, 0.995376, 0.01333034, 0.061030317]\n",
      "Epoch number: 160 ; Loss [1.0637589, 0.99400866, 0.009134332, 0.06061587]\n",
      "Epoch number: 161 ; Loss [1.090668, 0.99203336, 0.042853437, 0.05578108]\n",
      "Epoch number: 162 ; Loss [1.0784416, 0.98980856, 0.03420308, 0.054429993]\n",
      "Epoch number: 163 ; Loss [1.0700219, 0.9893547, 0.02815176, 0.052515335]\n",
      "Epoch number: 164 ; Loss [1.0677471, 0.9874905, 0.028236445, 0.052020196]\n",
      "Epoch number: 165 ; Loss [1.0653437, 0.98644847, 0.025003158, 0.053892106]\n",
      "Epoch number: 166 ; Loss [1.0561506, 0.9855225, 0.0100695435, 0.060558535]\n",
      "Epoch number: 167 ; Loss [1.0524411, 0.9858854, 0.003181001, 0.06337481]\n",
      "Epoch number: 168 ; Loss [1.0563818, 0.9809677, 0.012899618, 0.0625145]\n",
      "Epoch number: 169 ; Loss [1.0360184, 0.97227263, 0.007349711, 0.056395993]\n",
      "Epoch number: 170 ; Loss [1.0623306, 0.9735985, 0.040451277, 0.048280835]\n",
      "Epoch number: 171 ; Loss [1.0688051, 0.98785853, 0.03455527, 0.046391238]\n",
      "Epoch number: 172 ; Loss [1.0634171, 0.9863262, 0.030937582, 0.04615332]\n",
      "Epoch number: 173 ; Loss [1.0578146, 0.9901352, 0.007536128, 0.060143277]\n",
      "Epoch number: 174 ; Loss [1.079485, 0.9927738, 0.0151722785, 0.07153903]\n",
      "Epoch number: 175 ; Loss [1.0785353, 0.99350464, 0.0, 0.085030735]\n",
      "Epoch number: 176 ; Loss [1.0838581, 0.99387383, 0.0021095206, 0.0878748]\n",
      "Epoch number: 177 ; Loss [1.0771271, 0.9932289, 0.0, 0.083898224]\n",
      "Epoch number: 178 ; Loss [1.089509, 0.9918475, 0.02279596, 0.07486561]\n",
      "Epoch number: 179 ; Loss [1.0617956, 0.9883632, 0.011792116, 0.061640263]\n",
      "Epoch number: 180 ; Loss [1.0608838, 0.9877041, 0.022958022, 0.05022172]\n",
      "Epoch number: 181 ; Loss [1.0469948, 0.98293, 0.020585448, 0.04347936]\n",
      "Epoch number: 182 ; Loss [1.0472778, 0.9798898, 0.029105207, 0.03828277]\n",
      "Epoch number: 183 ; Loss [1.0331261, 0.9762198, 0.015014047, 0.04189224]\n",
      "Epoch number: 184 ; Loss [1.0556111, 0.9838681, 0.022622745, 0.049120337]\n",
      "Epoch number: 185 ; Loss [1.0503112, 0.9789751, 0.015030766, 0.056305304]\n",
      "Epoch number: 186 ; Loss [1.0441263, 0.9802797, 0.004215789, 0.05963085]\n",
      "Epoch number: 187 ; Loss [1.0585692, 0.9847743, 0.015313538, 0.058481347]\n",
      "Epoch number: 188 ; Loss [1.0658559, 0.98539305, 0.024246637, 0.056216117]\n",
      "Epoch number: 189 ; Loss [1.0526352, 0.982767, 0.014856355, 0.055011887]\n",
      "Epoch number: 190 ; Loss [1.0646154, 0.98350173, 0.022813331, 0.05830027]\n",
      "Epoch number: 191 ; Loss [1.0371004, 0.97336435, 0.007692962, 0.05604311]\n",
      "Epoch number: 192 ; Loss [1.0375112, 0.9696086, 0.014916501, 0.052986152]\n",
      "Epoch number: 193 ; Loss [1.0544089, 0.9671693, 0.038822763, 0.048416868]\n",
      "Epoch number: 194 ; Loss [1.0705125, 0.9683906, 0.05488524, 0.04723666]\n",
      "Epoch number: 195 ; Loss [1.0508609, 0.9669621, 0.03062283, 0.053275973]\n",
      "Epoch number: 196 ; Loss [1.0494447, 0.96733755, 0.022780396, 0.059326764]\n",
      "Epoch number: 197 ; Loss [1.05417, 0.9729072, 0.016606567, 0.06465629]\n",
      "Epoch number: 198 ; Loss [1.0520302, 0.9722178, 0.012329616, 0.06748274]\n",
      "Epoch number: 199 ; Loss [1.0664887, 0.97404003, 0.024652641, 0.0677961]\n",
      "Epoch number: 200 ; Loss [1.056068, 0.96586424, 0.01839314, 0.07181057]\n",
      "Epoch number: 201 ; Loss [1.0733299, 0.98126334, 0.018444732, 0.07362183]\n",
      "Epoch number: 202 ; Loss [1.0717974, 0.9811398, 0.019136734, 0.071520805]\n",
      "Epoch number: 203 ; Loss [1.0734344, 0.97269547, 0.024940807, 0.07579807]\n",
      "Epoch number: 204 ; Loss [1.0590885, 0.9762139, 0.020027168, 0.06284743]\n",
      "Epoch number: 205 ; Loss [1.0912682, 0.98145056, 0.05301462, 0.056802947]\n",
      "Epoch number: 206 ; Loss [1.0774121, 0.97682625, 0.048309047, 0.05227679]\n",
      "Epoch number: 207 ; Loss [1.0661627, 0.98337114, 0.026190238, 0.056601256]\n",
      "Epoch number: 208 ; Loss [1.0469452, 0.96768665, 0.022502154, 0.056756377]\n",
      "Epoch number: 209 ; Loss [1.0509615, 0.9668462, 0.027762514, 0.056352697]\n",
      "Epoch number: 210 ; Loss [1.0467725, 0.9709451, 0.015371575, 0.060455807]\n",
      "Epoch number: 211 ; Loss [1.0583357, 0.9776989, 0.022459101, 0.058177613]\n",
      "Epoch number: 212 ; Loss [1.0522542, 0.9835925, 0.011623709, 0.05703803]\n",
      "Epoch number: 213 ; Loss [1.0499083, 0.97425854, 0.025050836, 0.050598945]\n",
      "Epoch number: 214 ; Loss [1.0849853, 0.9774101, 0.059966512, 0.047608618]\n",
      "Epoch number: 215 ; Loss [1.080175, 0.9866457, 0.035135627, 0.05839375]\n",
      "Epoch number: 216 ; Loss [1.0718409, 0.9899474, 0.01501457, 0.066878945]\n",
      "Epoch number: 217 ; Loss [1.0625858, 0.9855758, 0.00097592734, 0.07603417]\n",
      "Epoch number: 218 ; Loss [1.0573031, 0.97951245, 0.0066944836, 0.071096115]\n",
      "Epoch number: 219 ; Loss [1.0431248, 0.9650885, 0.017975949, 0.060060397]\n",
      "Epoch number: 220 ; Loss [1.083898, 0.97846854, 0.05380988, 0.05161953]\n",
      "Epoch number: 221 ; Loss [1.0882281, 0.98268795, 0.05740118, 0.048139017]\n",
      "Epoch number: 222 ; Loss [1.0484561, 0.98199224, 0.0014099914, 0.06505379]\n",
      "Epoch number: 223 ; Loss [1.0667233, 0.98391986, 0.004631943, 0.07817156]\n",
      "Epoch number: 224 ; Loss [1.0760139, 0.98670566, 0.0099889375, 0.07931925]\n",
      "Epoch number: 225 ; Loss [1.0641311, 0.9789109, 0.0128297, 0.072390445]\n",
      "Epoch number: 226 ; Loss [1.1017835, 0.98163426, 0.056075834, 0.06407342]\n",
      "Epoch number: 227 ; Loss [1.0676539, 0.99602455, 0.0033532772, 0.06827604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 228 ; Loss [1.0680048, 0.9969384, 0.002572022, 0.068494394]\n",
      "Epoch number: 229 ; Loss [1.058224, 0.9947763, 0.0005028574, 0.06294483]\n",
      "Epoch number: 230 ; Loss [1.0968608, 0.99448776, 0.04406874, 0.058304355]\n",
      "Epoch number: 231 ; Loss [1.067036, 0.9947299, 0.013937909, 0.05836825]\n",
      "Epoch number: 232 ; Loss [1.0694932, 0.994335, 0.018736115, 0.056422073]\n",
      "Epoch number: 233 ; Loss [1.0520815, 0.9943087, 0.004149123, 0.053623628]\n",
      "Epoch number: 234 ; Loss [1.062543, 0.9935041, 0.019338425, 0.049700543]\n",
      "Epoch number: 235 ; Loss [1.0696961, 0.99244195, 0.033603728, 0.043650344]\n",
      "Epoch number: 236 ; Loss [1.0552614, 0.99163383, 0.01854428, 0.045083307]\n",
      "Epoch number: 237 ; Loss [1.0468587, 0.990368, 0.012987867, 0.043502767]\n",
      "Epoch number: 238 ; Loss [1.0377864, 0.99092454, 0.0055127623, 0.04134903]\n",
      "Epoch number: 239 ; Loss [1.0393022, 0.9900043, 0.010680899, 0.038617]\n",
      "Epoch number: 240 ; Loss [1.0715814, 0.99359655, 0.03987645, 0.03810836]\n",
      "Epoch number: 241 ; Loss [1.0381815, 0.9910209, 0.002796404, 0.044364247]\n",
      "Epoch number: 242 ; Loss [1.0401522, 0.9757425, 0.014685448, 0.049724266]\n",
      "Epoch number: 243 ; Loss [1.047443, 0.9754305, 0.024904754, 0.0471078]\n",
      "Epoch number: 244 ; Loss [1.0633978, 0.9935359, 0.03052755, 0.039334275]\n",
      "Epoch number: 245 ; Loss [1.0525825, 0.99411035, 0.008183925, 0.050288156]\n",
      "Epoch number: 246 ; Loss [1.0405904, 0.977417, 0.004934758, 0.05823867]\n",
      "Epoch number: 247 ; Loss [1.0486826, 0.9882979, 0.0027910727, 0.057593595]\n",
      "Epoch number: 248 ; Loss [1.0600989, 0.9978305, 0.014648709, 0.047619663]\n",
      "Epoch number: 249 ; Loss [1.0472093, 0.993635, 0.005124487, 0.04844971]\n",
      "Epoch number: 250 ; Loss [1.0254439, 0.96742034, 0.01177515, 0.04624838]\n",
      "Epoch number: 251 ; Loss [1.042858, 0.9941671, 0.013161303, 0.035529643]\n",
      "Epoch number: 252 ; Loss [1.0719948, 0.99353415, 0.045800686, 0.032659996]\n",
      "Epoch number: 253 ; Loss [1.0635751, 0.9810754, 0.04764806, 0.034851663]\n",
      "Epoch number: 254 ; Loss [1.0425836, 0.97518444, 0.015340583, 0.052058548]\n",
      "Epoch number: 255 ; Loss [1.0389112, 0.97215027, 0.008851353, 0.057909556]\n",
      "Epoch number: 256 ; Loss [1.0535995, 0.9904583, 0.0036824942, 0.05945868]\n",
      "Epoch number: 257 ; Loss [1.0819772, 0.9960556, 0.026585199, 0.05933642]\n",
      "Epoch number: 258 ; Loss [1.069053, 0.9954513, 0.022695087, 0.050906654]\n",
      "Epoch number: 259 ; Loss [1.0716896, 0.9896902, 0.0383533, 0.043646097]\n",
      "Epoch number: 260 ; Loss [1.0511804, 0.98713446, 0.018570598, 0.045475394]\n",
      "Epoch number: 261 ; Loss [1.0524657, 0.98506606, 0.02373313, 0.043666456]\n",
      "Epoch number: 262 ; Loss [1.04173, 0.9824511, 0.017888226, 0.041390784]\n",
      "Epoch number: 263 ; Loss [1.0461533, 0.99182034, 0.006707227, 0.047625687]\n",
      "Epoch number: 264 ; Loss [1.051255, 0.99193025, 0.00942192, 0.04990285]\n",
      "Epoch number: 265 ; Loss [1.0570499, 0.9939424, 0.012776174, 0.050331406]\n",
      "Epoch number: 266 ; Loss [1.057215, 0.9897953, 0.020552585, 0.046867035]\n",
      "Epoch number: 267 ; Loss [1.0443081, 0.9943228, 0.005298903, 0.044686325]\n",
      "Epoch number: 268 ; Loss [1.0490674, 0.9911133, 0.012733647, 0.045220397]\n",
      "Epoch number: 269 ; Loss [1.0535911, 0.988214, 0.026607817, 0.038769223]\n",
      "Epoch number: 270 ; Loss [1.0486555, 0.9963262, 0.014340955, 0.037988268]\n",
      "Epoch number: 271 ; Loss [1.0508366, 0.9912132, 0.026104718, 0.03351869]\n",
      "Epoch number: 272 ; Loss [1.0378941, 0.98871595, 0.010949741, 0.03822846]\n",
      "Epoch number: 273 ; Loss [1.0265168, 0.97271025, 0.017980292, 0.03582622]\n",
      "Epoch number: 274 ; Loss [1.0353335, 0.99075556, 0.007537256, 0.03704071]\n",
      "Epoch number: 275 ; Loss [1.0322291, 0.9891559, 0.0078066317, 0.035266567]\n",
      "Epoch number: 276 ; Loss [1.0495641, 0.9921626, 0.02261351, 0.034787968]\n",
      "Epoch number: 277 ; Loss [1.0710883, 0.98740554, 0.05173502, 0.031947736]\n",
      "Epoch number: 278 ; Loss [1.0349157, 0.9839149, 0.011992792, 0.039007932]\n",
      "Epoch number: 279 ; Loss [1.0595258, 0.9908818, 0.02629605, 0.042347983]\n",
      "Epoch number: 280 ; Loss [1.0312532, 0.98950034, 0.00046044285, 0.041292448]\n",
      "Epoch number: 281 ; Loss [1.0228031, 0.9856786, 0.00012878352, 0.036995627]\n",
      "Epoch number: 282 ; Loss [1.0654715, 0.98637056, 0.046929616, 0.03217142]\n",
      "Epoch number: 283 ; Loss [1.075996, 0.9935562, 0.052388143, 0.030051764]\n",
      "Epoch number: 284 ; Loss [1.0477208, 0.99603766, 0.009985932, 0.041697178]\n",
      "Epoch number: 285 ; Loss [1.0411026, 0.9915036, 0.0, 0.04959911]\n",
      "Epoch number: 286 ; Loss [1.0370278, 0.9861409, 0.0, 0.05088695]\n",
      "Epoch number: 287 ; Loss [1.044965, 0.98141897, 0.015391565, 0.04815445]\n",
      "Epoch number: 288 ; Loss [1.0485587, 0.9923912, 0.016005388, 0.040162038]\n",
      "Epoch number: 289 ; Loss [1.0572249, 0.9942277, 0.024972884, 0.038024314]\n",
      "Epoch number: 290 ; Loss [1.0593188, 0.9970968, 0.026260585, 0.035961337]\n",
      "Epoch number: 291 ; Loss [1.0376356, 0.99499005, 0.0014994706, 0.041145988]\n",
      "Epoch number: 292 ; Loss [1.0364891, 0.986102, 0.009242058, 0.041145056]\n",
      "Epoch number: 293 ; Loss [1.0308876, 0.9736059, 0.020001106, 0.03728064]\n",
      "Epoch number: 294 ; Loss [1.0704598, 0.99221396, 0.04253214, 0.03571383]\n",
      "Epoch number: 295 ; Loss [1.0573633, 0.9975562, 0.018837852, 0.040969286]\n",
      "Epoch number: 296 ; Loss [1.0719137, 0.99594134, 0.034518126, 0.041454196]\n",
      "Epoch number: 297 ; Loss [1.0342808, 0.9749663, 0.011217454, 0.048097]\n",
      "Epoch number: 298 ; Loss [1.0456529, 0.9817259, 0.016260585, 0.04766646]\n",
      "Epoch number: 299 ; Loss [1.0510561, 0.98848426, 0.014662087, 0.04790977]\n",
      "Epoch number: 300 ; Loss [1.0510489, 0.99709547, 0.0073242635, 0.04662922]\n",
      "Epoch number: 301 ; Loss [1.0449746, 0.99784553, 0.00073058973, 0.04639851]\n",
      "Epoch number: 302 ; Loss [1.0522163, 0.9945798, 0.016649513, 0.04098703]\n",
      "Epoch number: 303 ; Loss [1.05214, 0.9963734, 0.015745085, 0.040021498]\n",
      "Epoch number: 304 ; Loss [1.0524408, 0.99796987, 0.01614086, 0.03833007]\n",
      "Epoch number: 305 ; Loss [1.0423415, 0.9968318, 0.0037957563, 0.04171393]\n",
      "Epoch number: 306 ; Loss [1.05506, 0.9940052, 0.015407436, 0.04564734]\n",
      "Epoch number: 307 ; Loss [1.0719231, 0.9949544, 0.032298937, 0.044669703]\n",
      "Epoch number: 308 ; Loss [1.055739, 0.99531156, 0.014511564, 0.04591594]\n",
      "Epoch number: 309 ; Loss [1.0471554, 0.99726325, 0.0028558741, 0.04703629]\n",
      "Epoch number: 310 ; Loss [1.0474808, 0.9983889, 0.0048779165, 0.044214018]\n",
      "Epoch number: 311 ; Loss [1.0468063, 0.99864644, 0.0018088757, 0.046351083]\n",
      "Epoch number: 312 ; Loss [1.0403633, 0.9983496, 0.0, 0.042013694]\n",
      "Epoch number: 313 ; Loss [1.032612, 0.99295545, 0.0065699234, 0.033086576]\n",
      "Epoch number: 314 ; Loss [1.0412372, 0.99599457, 0.014758171, 0.030484455]\n",
      "Epoch number: 315 ; Loss [1.0772139, 0.9961989, 0.055036813, 0.025978241]\n",
      "Epoch number: 316 ; Loss [1.0430396, 0.99405336, 0.012501623, 0.036484566]\n",
      "Epoch number: 317 ; Loss [1.0613745, 0.9984913, 0.013700053, 0.04918326]\n",
      "Epoch number: 318 ; Loss [1.0637599, 0.9990917, 0.0040395223, 0.060628727]\n",
      "Epoch number: 319 ; Loss [1.0679543, 0.9991189, 0.0, 0.06883541]\n",
      "Epoch number: 320 ; Loss [1.0707402, 0.9979729, 0.0, 0.0727673]\n",
      "Epoch number: 321 ; Loss [1.064671, 0.9986398, 0.0, 0.066031225]\n",
      "Epoch number: 322 ; Loss [1.0589907, 0.99834585, 0.0028588753, 0.05778601]\n",
      "Epoch number: 323 ; Loss [1.0521973, 0.9961707, 0.00676205, 0.049264595]\n",
      "Epoch number: 324 ; Loss [1.0345258, 0.9873608, 0.009540938, 0.037624028]\n",
      "Epoch number: 325 ; Loss [1.028582, 0.9947821, 0.001237263, 0.03256265]\n",
      "Epoch number: 326 ; Loss [1.0492171, 0.99608356, 0.026015623, 0.02711791]\n",
      "Epoch number: 327 ; Loss [1.0518651, 0.9904905, 0.036476184, 0.024898376]\n",
      "Epoch number: 328 ; Loss [1.01525, 0.9644482, 0.025361778, 0.025439985]\n",
      "Epoch number: 329 ; Loss [1.025195, 0.9810656, 0.01837657, 0.02575279]\n",
      "Epoch number: 330 ; Loss [1.0412483, 0.9948555, 0.01637619, 0.030016616]\n",
      "Epoch number: 331 ; Loss [1.0337658, 0.9967278, 0.00038207462, 0.03665589]\n",
      "Epoch number: 332 ; Loss [1.0414332, 0.9964782, 0.0013076828, 0.043647364]\n",
      "Epoch number: 333 ; Loss [1.035788, 0.9878408, 0.0, 0.04794731]\n",
      "Epoch number: 334 ; Loss [1.0358217, 0.9824922, 0.006296214, 0.047033288]\n",
      "Epoch number: 335 ; Loss [1.035412, 0.98719716, 0.006912348, 0.04130244]\n",
      "Epoch number: 336 ; Loss [1.0581315, 0.9925406, 0.031358004, 0.034232818]\n",
      "Epoch number: 337 ; Loss [1.0527251, 0.98971176, 0.029537043, 0.033476293]\n",
      "Epoch number: 338 ; Loss [1.0306466, 0.98608994, 0.0042759376, 0.040280692]\n",
      "Epoch number: 339 ; Loss [1.0334644, 0.98949564, 0.0003613932, 0.04360742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 340 ; Loss [1.0564009, 0.99648076, 0.012580703, 0.04733949]\n",
      "Epoch number: 341 ; Loss [1.051736, 0.9945859, 0.010591547, 0.04655858]\n",
      "Epoch number: 342 ; Loss [1.0798702, 0.9913638, 0.042628504, 0.04587799]\n",
      "Epoch number: 343 ; Loss [1.066236, 0.9889448, 0.033764176, 0.043526966]\n",
      "Epoch number: 344 ; Loss [1.0474368, 0.99744225, 0.011410616, 0.038584046]\n",
      "Epoch number: 345 ; Loss [1.0363967, 0.9954171, 0.0, 0.040979683]\n",
      "Epoch number: 346 ; Loss [1.0226796, 0.97942144, 0.004995197, 0.038262963]\n",
      "Epoch number: 347 ; Loss [1.0910928, 0.9989056, 0.057046615, 0.03514064]\n",
      "Epoch number: 348 ; Loss [1.054203, 0.9942193, 0.022694888, 0.03728878]\n",
      "Epoch number: 349 ; Loss [1.0055519, 0.9609253, 0.0031570278, 0.041469634]\n",
      "Epoch number: 350 ; Loss [1.0426553, 0.9972172, 0.0020030055, 0.043435212]\n",
      "Epoch number: 351 ; Loss [1.0448184, 0.9989616, 0.0016431517, 0.044213615]\n",
      "Epoch number: 352 ; Loss [1.0404034, 0.997982, 0.0017830231, 0.040638268]\n",
      "Epoch number: 353 ; Loss [1.0390911, 0.99304986, 0.012938377, 0.033102885]\n",
      "Epoch number: 354 ; Loss [1.0383041, 0.9916428, 0.012822466, 0.033838928]\n",
      "Epoch number: 355 ; Loss [1.0549268, 0.99303216, 0.030349549, 0.031545043]\n",
      "Epoch number: 356 ; Loss [1.0740762, 0.9970914, 0.045819476, 0.031165201]\n",
      "Epoch number: 357 ; Loss [1.0341023, 0.996711, 0.00430423, 0.03308719]\n",
      "Epoch number: 358 ; Loss [1.0391389, 0.9975392, 0.0034248976, 0.038174786]\n",
      "Epoch number: 359 ; Loss [1.0326325, 0.99500525, 0.00026807422, 0.037359122]\n",
      "Epoch number: 360 ; Loss [1.0439929, 0.9964843, 0.009415756, 0.038092814]\n",
      "Epoch number: 361 ; Loss [1.0397004, 0.9972911, 0.0067308387, 0.035678506]\n",
      "Epoch number: 362 ; Loss [1.0477043, 0.9967483, 0.016581895, 0.03437417]\n",
      "Epoch number: 363 ; Loss [1.0419413, 0.9974545, 0.0092688985, 0.035217866]\n",
      "Epoch number: 364 ; Loss [1.0354916, 0.99631727, 0.004965745, 0.034208562]\n",
      "Epoch number: 365 ; Loss [1.0732578, 0.99391514, 0.047039382, 0.032303333]\n",
      "Epoch number: 366 ; Loss [1.0296222, 0.99110484, 0.005152622, 0.03336475]\n",
      "Epoch number: 367 ; Loss [1.0192851, 0.98733485, 0.0028995865, 0.029050604]\n",
      "Epoch number: 368 ; Loss [1.0215968, 0.98083043, 0.012770697, 0.027995609]\n",
      "Epoch number: 369 ; Loss [1.0465341, 0.99296975, 0.028696518, 0.024867825]\n",
      "Epoch number: 370 ; Loss [1.0465399, 0.99790823, 0.026418466, 0.022213167]\n",
      "Epoch number: 371 ; Loss [1.038158, 0.99704796, 0.014586334, 0.02652371]\n",
      "Epoch number: 372 ; Loss [1.0328208, 0.9924241, 0.0077234483, 0.032673378]\n",
      "Epoch number: 373 ; Loss [1.0175881, 0.9815572, 0.0, 0.0360309]\n",
      "Epoch number: 374 ; Loss [1.0100377, 0.9749787, 0.0, 0.035058945]\n",
      "Epoch number: 375 ; Loss [1.0300276, 0.99463177, 0.0058038123, 0.029592076]\n",
      "Epoch number: 376 ; Loss [1.0510093, 0.99726796, 0.027834062, 0.02590727]\n",
      "Epoch number: 377 ; Loss [1.0767865, 0.99703723, 0.05554106, 0.024208248]\n",
      "Epoch number: 378 ; Loss [1.0305897, 0.9949319, 0.0062707625, 0.02938717]\n",
      "Epoch number: 379 ; Loss [1.0140728, 0.97810173, 0.0, 0.03597108]\n",
      "Epoch number: 380 ; Loss [1.0200344, 0.97873056, 0.0, 0.041303914]\n",
      "Epoch number: 381 ; Loss [1.0344628, 0.9891546, 0.009525425, 0.035782814]\n",
      "Epoch number: 382 ; Loss [1.0436919, 0.9979296, 0.013149504, 0.03261275]\n",
      "Epoch number: 383 ; Loss [1.0475769, 0.99718153, 0.025040062, 0.025355333]\n",
      "Epoch number: 384 ; Loss [1.0414319, 0.9950206, 0.020977702, 0.025433542]\n",
      "Epoch number: 385 ; Loss [1.0384654, 0.99007446, 0.020976702, 0.027414164]\n",
      "Epoch number: 386 ; Loss [1.0323225, 0.99020535, 0.009757933, 0.032359287]\n",
      "Epoch number: 387 ; Loss [1.0399069, 0.98477525, 0.020440523, 0.03469115]\n",
      "Epoch number: 388 ; Loss [1.0486242, 0.9934176, 0.017315894, 0.037890665]\n",
      "Epoch number: 389 ; Loss [1.0571786, 0.99819237, 0.02366185, 0.035324484]\n",
      "Epoch number: 390 ; Loss [1.0385488, 0.9973408, 0.004230704, 0.036977276]\n",
      "Epoch number: 391 ; Loss [1.0395603, 0.998553, 0.0046237204, 0.036383584]\n",
      "Epoch number: 392 ; Loss [1.0460957, 0.9984398, 0.014703833, 0.03295202]\n",
      "Epoch number: 393 ; Loss [1.0324271, 0.99809325, 0.0018043099, 0.032529533]\n",
      "Epoch number: 394 ; Loss [1.0307684, 0.99565905, 0.0, 0.03510936]\n",
      "Epoch number: 395 ; Loss [1.0281954, 0.995046, 0.0034271607, 0.029722154]\n",
      "Epoch number: 396 ; Loss [1.0396944, 0.9940711, 0.017343001, 0.028280238]\n",
      "Epoch number: 397 ; Loss [1.03124, 0.99195856, 0.010144426, 0.029136956]\n",
      "Epoch number: 398 ; Loss [1.0409082, 0.9946012, 0.016943043, 0.02936402]\n",
      "Epoch number: 399 ; Loss [1.031542, 0.9947739, 0.0053453473, 0.031422563]\n",
      "Epoch number: 400 ; Loss [1.0461158, 0.99662805, 0.022418208, 0.027069433]\n",
      "Epoch number: 401 ; Loss [1.0392749, 0.99649066, 0.00989086, 0.032893464]\n",
      "Epoch number: 402 ; Loss [1.0315298, 0.99486244, 0.008042152, 0.028625283]\n",
      "Epoch number: 403 ; Loss [1.0351129, 0.9980982, 0.0057027377, 0.0313119]\n",
      "Epoch number: 404 ; Loss [1.0424427, 0.99593246, 0.018437942, 0.028072257]\n",
      "Epoch number: 405 ; Loss [1.0336524, 0.99829006, 0.004410361, 0.030951925]\n",
      "Epoch number: 406 ; Loss [1.0358429, 0.998, 0.0026106439, 0.035232212]\n",
      "Epoch number: 407 ; Loss [1.0302626, 0.9964161, 0.0, 0.03384645]\n",
      "Epoch number: 408 ; Loss [1.0273465, 0.99542034, 0.0, 0.031926144]\n",
      "Epoch number: 409 ; Loss [1.0259275, 0.996984, 0.002184301, 0.02675932]\n",
      "Epoch number: 410 ; Loss [1.0566787, 0.99929935, 0.032959495, 0.024419762]\n",
      "Epoch number: 411 ; Loss [1.0529077, 0.9976505, 0.029948328, 0.025308821]\n",
      "Epoch number: 412 ; Loss [1.030943, 0.99281263, 0.008882834, 0.02924751]\n",
      "Epoch number: 413 ; Loss [1.0258803, 0.98215604, 0.011271797, 0.032452516]\n",
      "Epoch number: 414 ; Loss [1.0198367, 0.98888505, 0.0, 0.030951668]\n",
      "Epoch number: 415 ; Loss [1.0404869, 0.99884117, 0.01274538, 0.028900346]\n",
      "Epoch number: 416 ; Loss [1.0362206, 0.9982959, 0.008078149, 0.029846506]\n",
      "Epoch number: 417 ; Loss [1.0316858, 0.992205, 0.008410655, 0.031070143]\n",
      "Epoch number: 418 ; Loss [1.0255243, 0.9910891, 0.0053545395, 0.029080607]\n",
      "Epoch number: 419 ; Loss [1.036745, 0.9956085, 0.014258688, 0.02687776]\n",
      "Epoch number: 420 ; Loss [1.0555062, 0.9976815, 0.032562405, 0.02526238]\n",
      "Epoch number: 421 ; Loss [1.032856, 0.9939325, 0.010785694, 0.028137812]\n",
      "Epoch number: 422 ; Loss [1.0260422, 0.9977579, 0.00034223823, 0.027942041]\n",
      "Epoch number: 423 ; Loss [1.0281073, 0.99322665, 0.0056734104, 0.029207192]\n",
      "Epoch number: 424 ; Loss [1.0258728, 0.993543, 0.0026345747, 0.029695237]\n",
      "Epoch number: 425 ; Loss [1.025604, 0.9974431, 0.0, 0.028160896]\n",
      "Epoch number: 426 ; Loss [1.0245235, 0.9948051, 0.004342637, 0.025375774]\n",
      "Epoch number: 427 ; Loss [1.0481756, 0.9970087, 0.03006439, 0.021102551]\n",
      "Epoch number: 428 ; Loss [1.0320036, 0.99692154, 0.013547229, 0.021534938]\n",
      "Epoch number: 429 ; Loss [0.9743394, 0.9458599, 0.0053174454, 0.023162086]\n",
      "Epoch number: 430 ; Loss [1.0327345, 0.9954304, 0.01503985, 0.022264274]\n",
      "Epoch number: 431 ; Loss [1.0730695, 0.95989794, 0.09407826, 0.019093286]\n",
      "Epoch number: 432 ; Loss [1.0496151, 0.9903696, 0.03253799, 0.026707496]\n",
      "Epoch number: 433 ; Loss [1.035397, 0.9987749, 0.0, 0.03662219]\n",
      "Epoch number: 434 ; Loss [1.0508511, 0.99946463, 0.0, 0.051386476]\n",
      "Epoch number: 435 ; Loss [1.0622003, 0.9994688, 0.0, 0.062731475]\n",
      "Epoch number: 436 ; Loss [1.0716395, 0.9995954, 0.0024946698, 0.069549486]\n",
      "Epoch number: 437 ; Loss [1.068921, 0.999248, 0.0, 0.06967289]\n",
      "Epoch number: 438 ; Loss [1.0815232, 0.9993435, 0.014743637, 0.067436025]\n",
      "Epoch number: 439 ; Loss [1.0773838, 0.9993355, 0.013951788, 0.06409639]\n",
      "Epoch number: 440 ; Loss [1.0603379, 0.9988512, 0.0052702674, 0.056216437]\n",
      "Epoch number: 441 ; Loss [1.045477, 0.99774086, 0.0, 0.047736175]\n",
      "Epoch number: 442 ; Loss [1.0374134, 0.9977312, 0.0, 0.039682124]\n",
      "Epoch number: 443 ; Loss [1.0496416, 0.99699366, 0.022085506, 0.030562403]\n",
      "Epoch number: 444 ; Loss [1.0295485, 0.9975783, 0.0058558118, 0.02611433]\n",
      "Epoch number: 445 ; Loss [1.0495555, 0.9987829, 0.02703694, 0.02373575]\n",
      "Epoch number: 446 ; Loss [1.0414786, 0.99909604, 0.019085767, 0.023296861]\n",
      "Epoch number: 447 ; Loss [1.0412759, 0.99848163, 0.02068963, 0.022104628]\n",
      "Epoch number: 448 ; Loss [1.0326297, 0.9984009, 0.0102483425, 0.02398048]\n",
      "Epoch number: 449 ; Loss [1.0303128, 0.9991007, 0.006267136, 0.02494488]\n",
      "Epoch number: 450 ; Loss [1.0313082, 0.99870086, 0.0052004047, 0.027406879]\n",
      "Epoch number: 451 ; Loss [1.0347564, 0.99888456, 0.005869955, 0.030001838]\n",
      "Epoch number: 452 ; Loss [1.0344765, 0.9991658, 0.0038052392, 0.03150545]\n",
      "Epoch number: 453 ; Loss [1.0388991, 0.99834496, 0.009407275, 0.031146927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 454 ; Loss [1.0284693, 0.99863863, 0.0, 0.02983068]\n",
      "Epoch number: 455 ; Loss [1.0380994, 0.9984102, 0.0106455255, 0.02904363]\n",
      "Epoch number: 456 ; Loss [1.025871, 0.9983063, 0.0, 0.027564725]\n",
      "Epoch number: 457 ; Loss [1.0301665, 0.99686944, 0.0050594555, 0.028237559]\n",
      "Epoch number: 458 ; Loss [1.0381776, 0.9961084, 0.016933776, 0.025135402]\n",
      "Epoch number: 459 ; Loss [1.0335171, 0.99652565, 0.013502451, 0.023489019]\n",
      "Epoch number: 460 ; Loss [1.0385625, 0.99443054, 0.025427688, 0.018704286]\n",
      "Epoch number: 461 ; Loss [1.0276147, 0.998545, 0.0070153475, 0.02205435]\n",
      "Epoch number: 462 ; Loss [1.0278254, 0.99894285, 0.0043200105, 0.024562534]\n",
      "Epoch number: 463 ; Loss [1.0248301, 0.99744153, 0.0026795373, 0.024708958]\n",
      "Epoch number: 464 ; Loss [1.0362085, 0.9989884, 0.00841539, 0.028804809]\n",
      "Epoch number: 465 ; Loss [1.0313615, 0.99930793, 0.0012550713, 0.03079848]\n",
      "Epoch number: 466 ; Loss [1.0283865, 0.99800724, 0.0, 0.030379267]\n",
      "Epoch number: 467 ; Loss [1.033483, 0.99920195, 0.0039619445, 0.03031906]\n",
      "Epoch number: 468 ; Loss [1.0339243, 0.99849427, 0.006703469, 0.028726518]\n",
      "Epoch number: 469 ; Loss [1.024019, 0.997074, 0.0014941341, 0.0254509]\n",
      "Epoch number: 470 ; Loss [1.0267155, 0.9984287, 0.003857948, 0.0244288]\n",
      "Epoch number: 471 ; Loss [1.0274893, 0.9986302, 0.007190937, 0.021668203]\n",
      "Epoch number: 472 ; Loss [1.023063, 0.99677813, 0.0080065, 0.01827833]\n",
      "Epoch number: 473 ; Loss [1.0235641, 0.99727994, 0.009175426, 0.017108677]\n",
      "Epoch number: 474 ; Loss [1.0336118, 0.99905366, 0.019040165, 0.0155179445]\n",
      "Epoch number: 475 ; Loss [1.0336907, 0.999352, 0.017591396, 0.016747355]\n",
      "Epoch number: 476 ; Loss [1.0247264, 0.9992346, 0.0072975466, 0.018194154]\n",
      "Epoch number: 477 ; Loss [1.027697, 0.9990419, 0.00825944, 0.02039569]\n",
      "Epoch number: 478 ; Loss [1.0199836, 0.9939001, 0.002382419, 0.02370115]\n",
      "Epoch number: 479 ; Loss [1.0230371, 0.996248, 0.000400719, 0.02638835]\n",
      "Epoch number: 480 ; Loss [1.0145686, 0.98537654, 0.0003197838, 0.028872237]\n",
      "Epoch number: 481 ; Loss [1.0283428, 0.9992937, 0.002816386, 0.026232708]\n",
      "Epoch number: 482 ; Loss [1.0263661, 0.9994084, 0.0044935658, 0.02246412]\n",
      "Epoch number: 483 ; Loss [1.0293424, 0.998025, 0.0076023703, 0.023714975]\n",
      "Epoch number: 484 ; Loss [0.99796695, 0.9670412, 0.007711756, 0.02321396]\n",
      "Epoch number: 485 ; Loss [1.0275127, 0.9971677, 0.0054849014, 0.024860032]\n",
      "Epoch number: 486 ; Loss [1.028351, 0.9988072, 0.005576984, 0.023966802]\n",
      "Epoch number: 487 ; Loss [1.0255029, 0.99254334, 0.005811219, 0.02714836]\n",
      "Epoch number: 488 ; Loss [1.0232596, 0.98178333, 0.014671016, 0.026805246]\n",
      "Epoch number: 489 ; Loss [1.0496237, 0.9954153, 0.030726155, 0.023482304]\n",
      "Epoch number: 490 ; Loss [1.0096506, 0.97302496, 0.012095356, 0.024530305]\n",
      "Epoch number: 491 ; Loss [1.0477123, 0.98595756, 0.037826814, 0.023927948]\n",
      "Epoch number: 492 ; Loss [1.0491365, 0.99750817, 0.02428162, 0.027346704]\n",
      "Epoch number: 493 ; Loss [1.0312561, 0.98974955, 0.0063787596, 0.035127703]\n",
      "Epoch number: 494 ; Loss [1.0350423, 0.9884869, 0.008057965, 0.03849739]\n",
      "Epoch number: 495 ; Loss [1.0425107, 0.99728775, 0.009453868, 0.035769153]\n",
      "Epoch number: 496 ; Loss [1.0233313, 0.9833539, 0.0029205326, 0.037056908]\n",
      "Epoch number: 497 ; Loss [1.0395328, 0.99132586, 0.017252438, 0.030954536]\n",
      "Epoch number: 498 ; Loss [1.0396695, 0.99838185, 0.015426033, 0.025861569]\n",
      "Epoch number: 499 ; Loss [1.0280495, 0.99660164, 0.0010007247, 0.030447097]\n"
     ]
    }
   ],
   "source": [
    "from Model import GAN\n",
    "\n",
    "Gan, G, D, F = GAN([28,28,1], 'B', 0.001, 1, 1)\n",
    "\n",
    "epochs=500\n",
    "batch_size=128\n",
    "\n",
    "target=9\n",
    "\n",
    "x_train_selected = x_train\n",
    "y_train_selected = y_train\n",
    "\n",
    "for cur_epoch in range(epochs):\n",
    "    for discrm_epoch in range(4):\n",
    "        input_batch = x_train_selected[np.random.randint(0, x_train_selected.shape[0], size=int(batch_size)),]\n",
    "        y_discrim = np.ones([batch_size,1])\n",
    "        if discrm_epoch % 2 == 0:\n",
    "            input_batch = np.add(input_batch, G.predict(input_batch))\n",
    "            y_discrim = np.zeros([batch_size,1])\n",
    "        D.train_on_batch(input_batch, y_discrim)\n",
    "        \n",
    "    input_idx = np.random.randint(0, x_train_selected.shape[0], size=int(batch_size))\n",
    "    input_batch = x_train_selected[input_idx,]\n",
    "    y_discrim = np.ones([batch_size,1])\n",
    "    #y_class = y_train_selected[input_idx,]\n",
    "    y_class=np_utils.to_categorical(np.ones(batch_size)*target, 10).astype(np.float32)\n",
    "    y_hinge=np.zeros([batch_size,28,28,1])\n",
    "    scalarloss=Gan.train_on_batch(input_batch, [y_discrim, y_class, y_hinge])\n",
    "    print(\"Epoch number:\",cur_epoch,\"; Loss\",scalarloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "idx = range(x_train.shape[0])\n",
    "#idx = np.random.randint(0, x_train_selected.shape[0], size=int(1000))\n",
    "real = x_train[idx,]\n",
    "fake = real + G.predict(real)\n",
    "pdt = np.argmax(Gan.predict(real)[1], axis=1)\n",
    "label = np.argmax(y_train[idx,], axis=1)\n",
    "idx = np.logical_and(pdt != label, pdt == target)\n",
    "real = real[idx,]\n",
    "fake = fake[idx,]\n",
    "pdt = pdt[idx,]\n",
    "label = label[idx,]\n",
    "idx = np.argsort(np.mean((real-fake)**2, axis=(1,2,3)))\n",
    "real = real[idx,]\n",
    "fake = fake[idx,]\n",
    "pdt = pdt[idx,]\n",
    "label = label[idx,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label.shape)\n",
    "print(pdt.shape)\n",
    "print(real.shape)\n",
    "print(fake.shape)\n",
    "\n",
    "np.save('samples/WB-B-t%d-label.npy' % target, label)\n",
    "np.save('samples/WB-B-t%d-target.npy' % target, pdt)\n",
    "np.save('samples/WB-B-t%d-clean.npy' % target, real)\n",
    "np.save('samples/WB-B-t%d-adv.npy' % target, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(10*label+pdt, return_counts=True)\n",
    "print('Num of types:', len(unique))\n",
    "print('Num of adv samples:', sum(counts))\n",
    "print('Statistics:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    print(label[k,])\n",
    "    print(pdt[k,])\n",
    "    plt.imshow((real[k,] * 255).astype(np.int).reshape(28,28), cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow((fake[k,] * 255).astype(np.int).reshape(28,28), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.zeros((280,280,1))\n",
    "\n",
    "c = {}\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        idx = np.logical_and(label==i, pdt==j)\n",
    "        if (10*i+j) in d and (10*i+j) not in c:\n",
    "            c[10*i+j] = fake[idx][0]\n",
    "        image[28*i:28*(i+1),28*j:28*(j+1),:] = c[10*i+j] if (10*i+j) in c else np.zeros((28,28,1))\n",
    "        \n",
    "print(len(c.keys()))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.clip(image*255, 0, 255).astype(np.int).reshape(280,280), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WB-C.pickle', 'rb') as handle:\n",
    "    record = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip(unique, counts))\n",
    "n = np.zeros((10,10))\n",
    "m = np.zeros((10,10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        n[i,j] = d[10*i+j] if (10*i+j) in d else int(i!=j)\n",
    "        idx = np.logical_and(label==i, pdt==j)\n",
    "        m[i,j] = np.median(np.mean((real[idx,]-fake[idx,])**2, axis=(1,2,3))) if (10*i+j) in d else float(i!=j)*0.07\n",
    "\n",
    "import seaborn\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel('adasd')\n",
    "n = np.log(n+1)*0.015\n",
    "seaborn.heatmap(n)\n",
    "#seaborn.heatmap(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
